{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ning/envs/anaconda3/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:44: UserWarning: You are currently using a nightly version of TensorFlow (2.4.0-dev20200705). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_model.ckpt.meta',\n",
       " 'bert_model.ckpt.data-00000-of-00001',\n",
       " 'bert_model.ckpt.index',\n",
       " 'bert_config.json',\n",
       " 'vocab.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_path = r\"/home/ning/BERT/uncased_L-12_H-768_A-12\"\n",
    "tf.io.gfile.listdir(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset glue/mrpc/1.0.0 (download: 1.43 MiB, generated: Unknown size, total: 1.43 MiB) to /home/ning/tensorflow_datasets/glue/mrpc/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ning/envs/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/home/ning/envs/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/home/ning/envs/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ning/tensorflow_datasets/glue/mrpc/1.0.0.incompleteM9D0HV/glue-train.tfrecord\n",
      "Shuffling and writing examples to /home/ning/tensorflow_datasets/glue/mrpc/1.0.0.incompleteM9D0HV/glue-validation.tfrecord\n",
      "Shuffling and writing examples to /home/ning/tensorflow_datasets/glue/mrpc/1.0.0.incompleteM9D0HV/glue-test.tfrecord\n",
      "\u001b[1mDataset glue downloaded and prepared to /home/ning/tensorflow_datasets/glue/mrpc/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "glue, info = tfds.load('glue/mrpc', with_info=True,\n",
    "                       # It's small, load the whole dataset\n",
    "                       batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(bert_path, \"vocab.txt\"),\n",
    "     do_lower_case=True)\n",
    "\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'you']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('I love you')\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "tokenizer.convert_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, tokenizer):\n",
    "    tokens = list(tokenizer.tokenize(s))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(glue_dict, tokenizer):\n",
    "    num_examples = len(glue_dict[\"sentence1\"])\n",
    "\n",
    "    sentence1 = tf.ragged.constant([\n",
    "      encode_sentence(s, tokenizer)\n",
    "      for s in np.array(glue_dict[\"sentence1\"])])\n",
    "    sentence2 = tf.ragged.constant([\n",
    "      encode_sentence(s, tokenizer)\n",
    "       for s in np.array(glue_dict[\"sentence2\"])])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "    \n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s1 = tf.zeros_like(sentence1)\n",
    "    type_s2 = tf.ones_like(sentence2)\n",
    "    input_type_ids = tf.concat(\n",
    "      [type_cls,type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "    inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "glue_train = bert_encode(glue['train'], tokenizer)\n",
    "glue_train_labels = glue['train']['label']\n",
    "\n",
    "glue_validation = bert_encode(glue['validation'], tokenizer)\n",
    "glue_validation_labels = glue['validation']['label']\n",
    "\n",
    "glue_test = bert_encode(glue['test'], tokenizer)\n",
    "glue_test_labels  = glue['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(3668, 103), dtype=int32, numpy=\n",
       " array([[ 101, 1996, 7235, ...,    0,    0,    0],\n",
       "        [ 101, 2625, 2084, ...,    0,    0,    0],\n",
       "        [ 101, 6804, 1011, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 9991, 2056, ...,    0,    0,    0],\n",
       "        [ 101, 1996, 5016, ...,    0,    0,    0],\n",
       "        [ 101, 2048, 1997, ...,    0,    0,    0]], dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(3668, 103), dtype=int32, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(3668, 103), dtype=int32, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0.1,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 512,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "bert_config_file = os.path.join(bert_path, \"bert_config.json\")\n",
    "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
    "\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
    "    bert_config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28989887,  0.26426142],\n",
       "       [ 0.15210018,  0.33391356],\n",
       "       [ 0.2511205 ,  0.24547443],\n",
       "       [ 0.18205245,  0.14981571],\n",
       "       [ 0.15887536, -0.06380289],\n",
       "       [ 0.1301665 ,  0.26489902],\n",
       "       [ 0.09217589,  0.05699844],\n",
       "       [ 0.400532  , -0.22095829],\n",
       "       [ 0.33746964,  0.26812357],\n",
       "       [ 0.19006425,  0.17356054]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_batch = {key: val[:10] for key, val in glue_train.items()}\n",
    "\n",
    "bert_classifier(\n",
    "    glue_batch, training=True\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up epochs and steps\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "train_data_size = len(glue_train_labels)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
    "\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "\n",
    "bert_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "115/115 [==============================] - 901s 8s/step - loss: 0.6425 - accuracy: 0.6636 - val_loss: 0.6293 - val_accuracy: 0.6838\n",
      "Epoch 2/3\n",
      "115/115 [==============================] - 899s 8s/step - loss: 0.6266 - accuracy: 0.6739 - val_loss: 0.6286 - val_accuracy: 0.6838\n",
      "Epoch 3/3\n",
      "115/115 [==============================] - 899s 8s/step - loss: 0.6085 - accuracy: 0.6884 - val_loss: 0.5958 - val_accuracy: 0.6838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f890d2deb50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_classifier.fit(\n",
    "      glue_train, glue_train_labels,\n",
    "      validation_data=(glue_validation, glue_validation_labels),\n",
    "      batch_size=32,\n",
    "      epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
