{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from transformers import TFBertModel,BertTokenizer,TFBertForSequenceClassification\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "emotionpush_data_path = r'/home/ning/dataset'\n",
    "bert_path = r\"/home/ning/bert_conf\"\n",
    "cache_dir=r'/home/ning/bert_conf/bert-base-uncased-cache'\n",
    "\n",
    "ckpt_path = r'/home/ning/models.ckpt'\n",
    "\n",
    "train_df = pd.read_pickle(os.path.join(emotionpush_data_path,'emotionpush_train.pkl'))\n",
    "test_df = pd.read_pickle(os.path.join(emotionpush_data_path,'emotionpush_test.pkl'))\n",
    "\n",
    "train_df = train_df[train_df['emotion'].isin([ 'neutral', 'joy', 'sadness', 'anger'])]\n",
    "test_df = test_df[test_df['emotion'].isin([ 'neutral', 'joy', 'sadness', 'anger'])]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(bert_path,'vocab_idea.txt'))\n",
    "# model = TFBertForSequenceClassification.from_pretrained(os.path.join(bert_path,'tf_model.h5'),config = os.path.join(bert_path,'config.json'),num_labels=4)\n",
    "# model = TFBertModel.from_pretrained(os.path.join(bert_path,'tf_model.h5'),config = os.path.join(bert_path,'config.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence_with_speaker(speaker,utterance,tokenizer,sos):\n",
    "    if sos:\n",
    "        tokens = ['[CLS]']\n",
    "    else:\n",
    "        tokens = []\n",
    "\n",
    "    if utterance in [';)',':)','<3',':(','LMAO','LOL','OMG','','lol','lmao','omg']:\n",
    "        return tokenizer.encode([utterance.upper()])\n",
    "        \n",
    "    \n",
    "    if speaker not in ['other','None']:\n",
    "        spk_token = '['+speaker+']'\n",
    "        tokens.append(spk_token)\n",
    "        tokens.append('[says]')\n",
    "        tokens.extend(list(tokenizer.tokenize(utterance)))\n",
    "        tokens.append('[SEP]')\n",
    "    \n",
    "    else:\n",
    "        tokens.extend(list(tokenizer.tokenize(utterance)))\n",
    "    \n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def bert_encode(dataframe, tokenizer,single=False):\n",
    "    \n",
    "    if not single:\n",
    "        num_examples = len(dataframe.index)\n",
    "        sentence1 = tf.ragged.constant([encode_sentence_with_speaker(s[0],s[1],tokenizer,True) for s in dataframe.values])\n",
    "\n",
    "        sentence2 = tf.ragged.constant([encode_sentence_with_speaker(s[2],s[3],tokenizer,False) for s in dataframe.values])\n",
    "\n",
    "        input_word_ids = tf.concat([sentence1, sentence2], axis=-1)\n",
    "\n",
    "        input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "        type_s1 = tf.zeros_like(sentence1)\n",
    "        type_s2 = tf.ones_like(sentence2)\n",
    "        input_type_ids = tf.concat([type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_word_ids.to_tensor(),\n",
    "            'attention_mask': input_mask,\n",
    "            'token_type_ids': input_type_ids}\n",
    "    \n",
    "    else:\n",
    "        num_examples = len(dataframe.index)\n",
    "        input_word_ids = tf.ragged.constant([encode_sentence_with_speaker(s[0],s[1],tokenizer,True) for s in dataframe.values])\n",
    "        \n",
    "        input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "        \n",
    "        input_type_ids = tf.zeros_like(input_word_ids).to_tensor()\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': input_word_ids.to_tensor(),\n",
    "            'attention_mask': input_mask,\n",
    "            'token_type_ids': input_type_ids}\n",
    "        \n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_df['emotion']\n",
    "test_target = test_df['emotion']\n",
    "\n",
    "\n",
    "train_features = bert_encode(train_df, tokenizer,single = False)\n",
    "test_features = bert_encode(test_df, tokenizer,single = False)\n",
    "\n",
    "ems = train_target.unique()\n",
    "def convert(emotion):\n",
    "    return np.where(ems == emotion)[0][0]\n",
    "\n",
    "train_labels = np.array(list(map(convert,train_target)))\n",
    "test_labels = np.array(list(map(convert,test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f_batch = {'input_ids': train_features['input_ids'][0:5],\n",
    "        'attention_mask': train_features['attention_mask'][0:5],\n",
    "        'token_type_ids': train_features['token_type_ids'][0:5]}\n",
    "train_l_batch = train_labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedBert(keras.layers.Layer):\n",
    "    def __init__(self,bert_file,bert_config):\n",
    "        super(PretrainedBert,self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_file,config = bert_config)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return self.bert(inputs)\n",
    "\n",
    "class ChatEmotion(keras.Model):\n",
    "    def __init__(self,bert_file,bert_config):\n",
    "        super(ChatEmotion, self).__init__()\n",
    "        self.bert = PretrainedBert(bert_file,bert_config)        \n",
    "        \n",
    "        self.dense = keras.layers.Dense(4, activation='softmax')\n",
    "    \n",
    "    def call(self,inputs,embedding = True):\n",
    "        cls_embeddings = self.bert(inputs)[0][:,0,:]\n",
    "        \n",
    "        if embedding:\n",
    "            return cls_embeddings\n",
    "        else:\n",
    "            cls = self.dense(cls_embeddings)\n",
    "            return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ning/bert_conf/tf_model.h5 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at /home/ning/bert_conf/tf_model.h5.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = ChatEmotion(os.path.join(bert_path,'tf_model.h5'),bert_config = os.path.join(bert_path,'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.32542852, 0.16106938, 0.3464705 , 0.1670316 ], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_f_batch,0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.31598407 0.34923843 0.23809561 0.09668195]\n",
      " [0.41697124 0.14555547 0.27276433 0.16470896]\n",
      " [0.287455   0.18570375 0.12795821 0.398883  ]\n",
      " [0.26985553 0.1652068  0.2148574  0.35008034]\n",
      " [0.47818822 0.18381248 0.10415643 0.2338429 ]], shape=(5, 4), dtype=float32)\n",
      "[1 0 3 3 0]\n",
      "tf.Tensor(1.2882814, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=2.5e-7)\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(train_f_batch,False)\n",
    "    print(prediction)\n",
    "    pred_label = np.argmax(prediction.numpy(),axis=1)\n",
    "    print(pred_label)\n",
    "    loss_value  = loss_object(y_true=train_l_batch, y_pred=prediction)\n",
    "    print(loss_value)\n",
    "\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ChatEmotion(keras.Model):\n",
    "    def __init__(self,bert_file,bert_config):\n",
    "        super(ChatEmotion, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_file,config = bert_config)        \n",
    "        self.dense = keras.layers.Dense(4, activation='softmax')\n",
    "    \n",
    "    def call(self,inputs,embedding = True):\n",
    "        cls_embeddings = self.bert(inputs)[0][:,0,:]\n",
    "        \n",
    "        if embedding:\n",
    "            return cls_embeddings\n",
    "        else:\n",
    "            cls = self.dense(cls_embeddings)\n",
    "            return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ning/bert_conf/tf_model.h5 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at /home/ning/bert_conf/tf_model.h5.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = ChatEmotion(os.path.join(bert_path,'tf_model.h5'),bert_config = os.path.join(bert_path,'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.16600533 0.11135951 0.24714075 0.4754944 ]\n",
      " [0.0879107  0.15858485 0.13441016 0.61909425]\n",
      " [0.1681401  0.11430804 0.22191021 0.49564165]\n",
      " [0.14224131 0.09006726 0.16430868 0.6033827 ]\n",
      " [0.09710054 0.13526304 0.1330557  0.63458073]], shape=(5, 4), dtype=float32)\n",
      "[3 3 3 3 3]\n",
      "tf.Tensor(1.5230981, shape=(), dtype=float32)\n",
      "[0 0 0 0 0]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for _ in range(5):\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=2.5e-7)\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(train_f_batch,False)\n",
    "    print(prediction)\n",
    "    pred_label = np.argmax(prediction.numpy(),axis=1)\n",
    "    print(pred_label)\n",
    "    loss_value  = loss_object(y_true=train_l_batch, y_pred=prediction)\n",
    "    print(loss_value)\n",
    "    print(train_l_batch)\n",
    "\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedBert(keras.layers.Layer):\n",
    "    def __init__(self,bert_file,bert_config):\n",
    "        super(PretrainedBert,self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_file,config = bert_config)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return self.bert(inputs)\n",
    "\n",
    "class ChatEmotion(keras.Model):\n",
    "    def __init__(self,bert_file,bert_config):\n",
    "        super(ChatEmotion, self).__init__()\n",
    "        self.bert = PretrainedBert(bert_file,bert_config)        \n",
    "        \n",
    "        self.dense = keras.layers.Dense(4, activation='softmax')\n",
    "    \n",
    "    def call(self,inputs,embedding = True):\n",
    "        cls_embeddings = self.bert(inputs)[0][:,0,:]\n",
    "        \n",
    "        if embedding:\n",
    "            return cls_embeddings\n",
    "        else:\n",
    "            cls = self.dense(cls_embeddings)\n",
    "            return cls\n",
    "\n",
    "\n",
    "class ChatEmotion(keras.Model):\n",
    "    def __init__(self,bert_file,bert_config):\n",
    "        super(ChatEmotion, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(bert_file,config = bert_config)        \n",
    "        self.dense = keras.layers.Dense(4, activation='softmax')\n",
    "    \n",
    "    def call(self,inputs,embedding = True):\n",
    "        cls_embeddings = self.bert(inputs)[0][:,0,:]\n",
    "        \n",
    "        if embedding:\n",
    "            return cls_embeddings\n",
    "        else:\n",
    "            cls = self.dense(cls_embeddings)\n",
    "            return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ning/bert_conf/tf_model.h5 were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at /home/ning/bert_conf/tf_model.h5 and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "cls = TFBertForSequenceClassification.from_pretrained(os.path.join(bert_path,'tf_model.h5'),config = os.path.join(bert_path,'config.json'),num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 768), dtype=float32, numpy=\n",
       "array([[-0.23915076, -0.25951898,  0.03390481, ..., -0.11725874,\n",
       "         0.21749967,  0.6405008 ],\n",
       "       [-0.4463973 , -0.03729638,  0.35437933, ..., -0.33169132,\n",
       "         0.2951671 ,  0.41419148],\n",
       "       [-0.03577711, -0.19770059,  0.51548743, ..., -0.5142569 ,\n",
       "         0.17428611,  0.31457123],\n",
       "       [-0.02149344, -0.40951985,  0.4850874 , ..., -0.5744982 ,\n",
       "        -0.05005437,  0.6261888 ],\n",
       "       [ 0.31917137, -0.03037216,  0.8768659 , ..., -0.03596294,\n",
       "        -0.06128835, -0.20560619]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.layers[0](train_f_batch)[0][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.modeling_tf_bert.TFBertForSequenceClassification at 0x7f9f44112890>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=2.5e-7)\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = cls.layers[0](train_f_batch)[0][:,0,:]\n",
    "    pred_label = np.argmax(prediction.numpy(),axis=1)\n",
    "    loss_value  = loss_object(y_true=train_l_batch, y_pred=prediction)\n",
    "    print(loss_value)\n",
    "    print(train_l_batch)\n",
    "\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([-0.23915076, -0.25951898,  0.03390481, -0.15077853, -0.57236046],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.layers[0](train_f_batch)[0][0,0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.modeling_tf_bert.TFBertMainLayer at 0x7fca6d7518d0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fca6b284d90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fcacd7ba8d0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接套TFBertForSequenceClassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertPreTrainedModel\n",
    "\n",
    "class ChatEmotion(keras.Model):\n",
    "    def __init__(self, bert_file, config, num_labels):\n",
    "        super(ChatEmotion,self).__init__()\n",
    "        self.bert = TFBertForSequenceClassification.from_pretrained(os.path.join(bert_path,'tf_model.h5'),config = os.path.join(bert_path,'config.json'),num_labels=num_labels)\n",
    "        self.bert_main_layer = self.bert.layers[0]\n",
    "        \n",
    "        \n",
    "    def call(self,inputs,embedding = True):\n",
    "    \n",
    "        if embedding:\n",
    "            return self.bert_main_layer(inputs)[1]\n",
    "        else:\n",
    "            return self.bert(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ning/bert_conf/tf_model.h5 were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at /home/ning/bert_conf/tf_model.h5 and are newly initialized: ['dropout_75', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ChatEmotion(os.path.join(bert_path,'tf_model.h5'),config = os.path.join(bert_path,'config.json'),num_labels = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_f_batch,1).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7025588, shape=(), dtype=float32)\n",
      "[0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=2.5e-5)\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(train_f_batch,0)[0]\n",
    "    pred_label = np.argmax(prediction.numpy(),axis=1)\n",
    "    loss_value  = loss_object(y_true=train_l_batch, y_pred=prediction)\n",
    "    print(loss_value)\n",
    "    print(train_l_batch)\n",
    "\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01750542, -0.02568091, -0.0366416 , -0.02528609,  0.007971  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()[1][0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].weights[0] is model.layers[0].layers[0].weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.modeling_tf_bert.TFBertForSequenceClassification at 0x7ff8fc2004f0>,\n",
       " <transformers.modeling_tf_bert.TFBertMainLayer at 0x7ff8fc20a190>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([30522, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 继承TFBertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertPreTrainedModel\n",
    "\n",
    "class BertQAModel(TFBertPreTrainedModel):\n",
    "    \n",
    "    DROPOUT_RATE = 0.1\n",
    "    NUM_HIDDEN_STATES = 2\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.concat = L.Concatenate()\n",
    "        self.dropout = L.Dropout(self.DROPOUT_RATE)\n",
    "        self.qa_outputs = L.Dense(\n",
    "            config.num_labels, \n",
    "            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "            dtype='float32',\n",
    "            name=\"qa_outputs\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # outputs: Tuple[sequence, pooled, hidden_states]\n",
    "        _, _, hidden_states = self.bert(inputs, **kwargs)\n",
    "        \n",
    "        hidden_states = self.concat([\n",
    "            hidden_states[-i] for i in range(1, self.NUM_HIDDEN_STATES+1)\n",
    "        ])\n",
    "        \n",
    "        hidden_states = self.dropout(hidden_states, training=kwargs.get(\"training\", False))\n",
    "        logits = self.qa_outputs(hidden_states)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
